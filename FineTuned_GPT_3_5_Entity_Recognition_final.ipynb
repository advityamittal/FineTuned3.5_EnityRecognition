{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM8MRkf8Dr94"
      },
      "source": [
        "# Entity recognition Fine-tuned GPT-3.5-turbo\n",
        "Author : [Advitya Mittal]\n",
        "\n",
        "Experimental conceptual pipeline to train a high-performing task-specific model using a pre-trained model for fine-tuning As well as Data generation using a Single Prompt as the only input.\n",
        "\n",
        "\n",
        "Generating high-quality and diverse training examples based on a given prompt, fine-tuning an OpenAI model with these examples, and validating the fine-tuned model's capabilities. This end-to-end pipeline demonstrates the use of the OpenAI API, robust error handling, and efficient data manipulation.\n",
        "\n",
        "\n",
        "Adapted from [Matt Shumer](https://github.com/mshumer)'s work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Way3_PuPpIuE"
      },
      "source": [
        "#Data generation step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-3DvlIpVSl"
      },
      "source": [
        "Write your prompt here. Make it as descriptive as possible!\n",
        "\n",
        "Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.\n",
        "\n",
        "### One prompt -> fine-tuned GPT-3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7WKZyxtpUPS"
      },
      "outputs": [],
      "source": [
        "prompt = \"Identify the entities in ANY and ALL given inputs, categorize and return the identified entities into 'Person', 'Location', 'Organization' and 'Date' columns in that order. ONLY return this categorization.\"\n",
        "temperature = .5\n",
        "number_of_examples = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyjVn-XXQpD3"
      },
      "source": [
        "####Dependencies\n",
        "The project relies on openai for accessing GPT-3.5 API, tenacity for implementing retry logic in API calls, and pandas for data manipulation and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuL2UaqlsmBD",
        "outputId": "827ca8f4-38f8-42a8-838d-00c178eb9a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere\n",
            "  Downloading cohere-4.46-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.3)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: importlib_metadata, fastavro, backoff, tiktoken, cohere\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.46 fastavro-1.9.3 importlib_metadata-6.11.0 tiktoken-0.5.2\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken cohere\n",
        "!pip install openai==0.28\n",
        "!pip install tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdsd82ngpHCG",
        "outputId": "43715e02-569b-4cfa-d752-4c8ca19ab386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating example 0\n",
            "Generating example 1\n",
            "Generating example 2\n",
            "Generating example 3\n",
            "Generating example 4\n",
            "Generating example 5\n",
            "Generating example 6\n",
            "Generating example 7\n",
            "Generating example 8\n",
            "Generating example 9\n",
            "Generating example 10\n",
            "Generating example 11\n",
            "Generating example 12\n",
            "Generating example 13\n",
            "Generating example 14\n",
            "['```\\nprompt\\n-----------\\nIdentify the entities in the following sentence: \"John and Mary went to Paris in June.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: John, Mary\\nLocation: Paris\\nOrganization: \\nDate: June\\n-----------', '```\\nprompt\\n-----------\\nExtract and categorize the entities from this text: \"Google announced its new CEO, Sundar Pichai, on August 10th, 2015, in Mountain View.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: Sundar Pichai\\nLocation: Mountain View\\nOrganization: Google\\nDate: August 10th, 2015\\n-----------', '```\\nprompt\\n-----------\\nFrom the given information, classify the entities: \"During the 2020 Tokyo Olympics, athletes from around the world competed in Japan, with the event being sponsored by Coca-Cola.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: Tokyo, Japan\\nOrganization: Coca-Cola\\nDate: 2020\\n-----------', '```\\nprompt\\n-----------\\nCategorize the entities in this sentence: \"Michelle Obama released her book \\'Becoming\\' in November 2018, which quickly became a bestseller in the United States.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: Michelle Obama\\nLocation: United States\\nOrganization: \\nDate: November 2018\\n-----------', '```\\nprompt\\n-----------\\nIdentify and categorize the entities in the following statement: \"The Nobel Peace Prize for 2021 was awarded to journalists Maria Ressa and Dmitry Muratov for their efforts to safeguard freedom of expression in the Philippines and Russia, respectively.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: Maria Ressa, Dmitry Muratov\\nLocation: Philippines, Russia\\nOrganization: \\nDate: 2021\\n-----------', '```\\nprompt\\n-----------\\nFrom the text provided, extract and categorize the entities: \"Tesla, Inc. announced the opening of its new factory in Austin, Texas, on July 7th, 2021, aiming to increase production of electric vehicles.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: Austin, Texas\\nOrganization: Tesla, Inc.\\nDate: July 7th, 2021\\n-----------', '```\\nprompt\\n-----------\\nAnalyze and categorize the entities in this information: \"In a historic meeting on April 27th, 2018, leaders from North and South Korea met at the Demilitarized Zone to discuss peace initiatives.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: Demilitarized Zone, North Korea, South Korea\\nOrganization: \\nDate: April 27th, 2018\\n-----------', '```\\nprompt\\n-----------\\nClassify the entities in the sentence: \"The World Health Organization declared COVID-19 a global pandemic on March 11th, 2020, urging countries worldwide to take immediate actions.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: \\nOrganization: World Health Organization\\nDate: March 11th, 2020\\n-----------', '```\\nprompt\\n-----------\\nExtract and categorize the entities from the following text: \"Harvard University, established in 1636, is located in Cambridge, Massachusetts, and is one of the oldest institutions of higher education in the United States.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: Cambridge, Massachusetts, United States\\nOrganization: Harvard University\\nDate: 1636\\n-----------', '```\\nprompt\\n-----------\\nIdentify and categorize the entities in this statement: \"Barack Obama, the 44th President of the United States, was awarded the Nobel Peace Prize in 2009 for his extraordinary efforts to strengthen international diplomacy and cooperation between peoples.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: Barack Obama\\nLocation: United States\\nOrganization: \\nDate: 2009\\n-----------', '```\\nprompt\\n-----------\\nIdentify and categorize the entities in this narrative: \"Leonardo DiCaprio, a renowned actor, collaborated with Martin Scorsese on the film \\'The Wolf of Wall Street\\', which premiered in New York City on December 17, 2013.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: Leonardo DiCaprio, Martin Scorsese\\nLocation: New York City\\nOrganization: \\nDate: December 17, 2013\\n-----------', '```\\nprompt\\n-----------\\nExtract and categorize the entities from this announcement: \"The Nobel Prize in Literature for the year 2021 was awarded to Abdulrazak Gurnah for his influential writings that explore the effects of colonialism.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: Abdulrazak Gurnah\\nLocation: \\nOrganization: Nobel Prize\\nDate: 2021\\n-----------', '```\\nprompt\\n-----------\\nExtract and categorize the entities from the following: \"The Battle of Gettysburg, a pivotal encounter during the American Civil War, took place from July 1 to July 3, 1863, in Pennsylvania.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: Gettysburg, Pennsylvania\\nOrganization: \\nDate: July 1 to July 3, 1863\\n-----------', '```\\nprompt\\n-----------\\nFrom the provided text, identify and categorize the entities: \"Amazon acquired Whole Foods Market for $13.7 billion on June 16, 2017, marking a significant expansion into the grocery sector.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: \\nOrganization: Amazon, Whole Foods Market\\nDate: June 16, 2017\\n-----------', '```\\nprompt\\n-----------\\nFrom the text, identify and categorize the entities: \"Microsoft announced the acquisition of GitHub, a leading software development platform, for $7.5 billion in stock on June 4, 2018.\"\\n-----------\\n\\nresponse\\n-----------\\nPerson: \\nLocation: \\nOrganization: Microsoft, GitHub\\nDate: June 4, 2018\\n-----------']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import random\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "openai.api_key = \"sk\"\n",
        "N_RETRIES = 3\n",
        "\n",
        "## Generates data samples based on the provided prompt, with complexity increasing for each example.\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def generate_example(prompt, prev_examples, temperature=.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 8:\n",
        "            prev_examples = random.sample(prev_examples, 8)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-0125-preview\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=400,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# Generate examples\n",
        "prev_examples = []\n",
        "for i in range(number_of_examples):\n",
        "    print(f'Generating example {i}')\n",
        "    example = generate_example(prompt, prev_examples, temperature)\n",
        "    prev_examples.append(example)\n",
        "\n",
        "print(prev_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC6iJzXjugJ-"
      },
      "source": [
        "We also need to generate a system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMcfhW6Guh2E",
        "outputId": "2db87d95-475f-41fb-857f-e44e7f188fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The system message is: `Given a text, identify and categorize the entities into 'Person', 'Location', 'Organization' and 'Date' columns.`. Feel free to re-run this cell if you want a better result.\n"
          ]
        }
      ],
      "source": [
        "def generate_system_message(prompt):\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt.strip(),\n",
        "          }\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "system_message = generate_system_message(prompt)\n",
        "\n",
        "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6BqZ-hjseBF"
      },
      "source": [
        "Now let's put our examples into a dataframe and turn them into a final pair of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CEdkYeRsdmB",
        "outputId": "9a59829e-18cf-43c6-e66e-68d4852a9f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 15 successfully-generated examples.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store prompts and responses\n",
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "# Parse out prompts and responses from examples\n",
        "for example in prev_examples:\n",
        "  try:\n",
        "    split_example = example.split('-----------')\n",
        "    prompts.append(split_example[1].strip())\n",
        "    responses.append(split_example[3].strip())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'response': responses\n",
        "})\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print('There are ' + str(len(df)) + ' successfully-generated examples.')\n",
        "\n",
        "# Initialize list to store training examples\n",
        "training_examples = []\n",
        "\n",
        "# Create training examples in the format required for GPT-3.5 fine-tuning\n",
        "for index, row in df.iterrows():\n",
        "    training_example = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_message.strip()},\n",
        "            {\"role\": \"user\", \"content\": row['prompt']},\n",
        "            {\"role\": \"assistant\", \"content\": row['response']}\n",
        "        ]\n",
        "    }\n",
        "    training_examples.append(training_example)\n",
        "\n",
        "# Save training examples to a .jsonl file\n",
        "with open('training_examples.jsonl', 'w') as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWTY6qVgXD_T"
      },
      "source": [
        "# Upload the file to OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LjEUrI9XDgT"
      },
      "outputs": [],
      "source": [
        "file_id = openai.File.create(\n",
        "  file=open(\"/content/training_examples.jsonl\", \"rb\"),\n",
        "  purpose='fine-tune'\n",
        ").id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYRIq8dW9IR"
      },
      "source": [
        "# Train the model! You may need to wait a few minutes before running the next cell to allow for the file to process on OpenAI's servers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdEyXmkoW80I"
      },
      "outputs": [],
      "source": [
        "job = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "job_id = job.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUSX5QzmZMTd"
      },
      "source": [
        "# Now, just wait until the fine-tuning run is done, and you'll have a ready-to-use model!\n",
        "\n",
        "Run this cell every 20 minutes or so -- eventually, you'll see a message \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:xxxxxxxxxxxx\"\n",
        "\n",
        "Once you see that message, you can go to the OpenAI Playground (or keep going to the next cells and use the API) to try the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DJZ7hHaBx0",
        "outputId": "611818bc-0f34-4c9f-ef71-fe9184185987"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7d5b84938e00> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-bNaeBqFkAFyCA2wtc1yROflD\",\n",
              "      \"created_at\": 1707437290,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"The job has successfully completed\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-TYyPWMK7JlRSNbQIEsEe8MxK\",\n",
              "      \"created_at\": 1707437288,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:personal::8q8j12n0\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-REH0vnDh1wvYQ4cGmBmEcaLX\",\n",
              "      \"created_at\": 1707437280,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 90/90: training loss=0.00\",\n",
              "      \"data\": {\n",
              "        \"step\": 90,\n",
              "        \"train_loss\": 3.655751470432733e-06,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-IYL2kgv0hV8n1Lb8gTALnlZP\",\n",
              "      \"created_at\": 1707437278,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 89/90: training loss=0.00\",\n",
              "      \"data\": {\n",
              "        \"step\": 89,\n",
              "        \"train_loss\": 4.57763690064894e-06,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-nybGyU3THC0yJaiGn6Q0XjEu\",\n",
              "      \"created_at\": 1707437276,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 88/90: training loss=0.00\",\n",
              "      \"data\": {\n",
              "        \"step\": 88,\n",
              "        \"train_loss\": 4.309194991947152e-06,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-5B83HDL0hezJvINZFdcitTE5\",\n",
              "      \"created_at\": 1707437273,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 87/90: training loss=0.00\",\n",
              "      \"data\": {\n",
              "        \"step\": 87,\n",
              "        \"train_loss\": 2.5033950805664062e-06,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-LpP4mh2HWcevjg4VukHiDobS\",\n",
              "      \"created_at\": 1707437271,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 86/90: training loss=0.00\",\n",
              "      \"data\": {\n",
              "        \"step\": 86,\n",
              "        \"train_loss\": 4.57763690064894e-06,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-Xcj2XQDh1aOxDGiFv3Y1jA8j\",\n",
              "      \"created_at\": 1707437269,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 85/90: training loss=0.02\",\n",
              "      \"data\": {\n",
              "        \"step\": 85,\n",
              "        \"train_loss\": 0.016783475875854492,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-rs5VbJHDUwa9V22vSPkqzXiX\",\n",
              "      \"created_at\": 1707437266,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 84/90: training loss=0.00\",\n",
              "      \"data\": {\n",
              "        \"step\": 84,\n",
              "        \"train_loss\": 4.995436938770581e-06,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-y4eoW1ERkG5oY2qB8v7hTdVB\",\n",
              "      \"created_at\": 1707437264,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 83/90: training loss=0.00\",\n",
              "      \"data\": {\n",
              "        \"step\": 83,\n",
              "        \"train_loss\": 4.132588583161123e-05,\n",
              "        \"total_steps\": 90,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    }\n",
              "  ],\n",
              "  \"has_more\": true\n",
              "}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai.FineTuningJob.list_events(id=job_id, limit=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ihW2O27Phl"
      },
      "source": [
        "# Once your model is trained, run the next cell to grab the fine-tuned model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWBRBPh8aEzH",
        "outputId": "2d1aa247-4fcf-4a47-af30-20d6d3cb8dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ft:gpt-3.5-turbo-0613:personal::8q8j12n0\n"
          ]
        }
      ],
      "source": [
        "model_name_pre_object = openai.FineTuningJob.retrieve(job_id)\n",
        "model_name = model_name_pre_object.fine_tuned_model\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OmZLoBX7oQM"
      },
      "source": [
        "# Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uxbrmzc5dMuC",
        "outputId": "39429af7-6169-4042-b9ad-2466a7e26ac7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Person: Jack, Jill, Jordan\\nLocation: hill, Jordan\\nOrganization: \\nDate:'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Jack, Jill and Jordan went up the hill in Jordan.\",\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3n7TWHyWhhfd",
        "outputId": "e7b3672a-c8d0-4412-cf68-99515eb301d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Person: Dr. Emily Stone\\nLocation: University of Oxford, Geneva\\nOrganization: \\nDate: September 15th, 2023'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Dr. Emily Stone, a renowned biologist from the University of Oxford, will be presenting her research on climate change in Geneva on September 15th, 2023\",\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dR6c0UI3g1jY",
        "outputId": "7ac9f436-bd80-4c71-b712-24a0f91400a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Person: \\nLocation: \\nOrganization: \\nDate:'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#  No entities\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Compare the goals in soccer to the objectives of a corporate strategy meeting\",\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4_bZffLNiG3m",
        "outputId": "d38b23d1-0ca4-469e-9911-b969564fa3da"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Person: \\nLocation: parallel universe\\nOrganization: \\nDate:'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#  Abstract and Philosophical\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"If a marathon was run in a parallel universe where time flows backward, who would be considered the winner?\",\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HdHi8DUwiaKq",
        "outputId": "8681ec02-3db0-4128-d694-94c11e3a6a07"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Person: \\nLocation: \\nOrganization: \\nDate:'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#  IRRELEVANT - No entities\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Investigate the role of photosynthesis in plants.\", # virtual reality space exploration experiences\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iXxbAfa9q_mW",
        "outputId": "16f836f6-ed20-495d-edac-7b76d224ee0b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Yes, I'm still here. Please provide the text for entity identification and categorization.\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"still working ?\",\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BbbiMC8ES1Mj",
        "outputId": "2064b646-e3b1-44c6-fa2a-e2a5608af336"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Person: Anil\\nLocation: Apple headquarters, cafeteria\\nOrganization: Apple\\nDate: yesterday'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"I visited the Apple headquarters yesterday and met my friend Anil in their cafeteria\",\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message['content']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
